{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tabular Deep Learning Models for Medical Diagnosis\n",
        "\n",
        "## Experiment Overview\n",
        "\n",
        "This notebook trains two state-of-the-art tabular deep learning algorithms:\n",
        "1. **TabNet** - Interpretable deep learning model using sequential attention mechanism\n",
        "2. **FT-Transformer** - Feature Tokenization + Transformer architecture for tabular data\n",
        "\n",
        "### Dataset\n",
        "- Using pre-filtered datasets (evidence features already encoded, raw columns removed)\n",
        "- Training: preprocessed_filtered/train_filtered.csv\n",
        "- Validation: preprocessed_filtered/validation_filtered.csv\n",
        "- Test: preprocessed_filtered/test_filtered.csv\n",
        "\n",
        "### Evaluation Metrics\n",
        "- Accuracy, Macro F1-score, Weighted F1-score, Top-3 Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Custom FT-Transformer implementation loaded (PyTorch 2.x compatible)\n",
            "✓ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "# Model libraries\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "except ImportError:\n",
        "    print(\"⚠️  pytorch-tabnet not installed. Install with: pip install pytorch-tabnet\")\n",
        "    TabNetClassifier = None\n",
        "\n",
        "# Custom FT-Transformer implementation (PyTorch 2.x compatible)\n",
        "class FTTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    FT-Transformer: Feature Tokenization + Transformer for tabular data\n",
        "    Based on: \"Revisiting Deep Learning Models for Tabular Data\" (Gorishniy et al., 2021)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_num_features,\n",
        "        cat_cardinalities=None,\n",
        "        d_token=192,\n",
        "        n_blocks=3,\n",
        "        attention_dropout=0.2,\n",
        "        ffn_dropout=0.1,\n",
        "        residual_dropout=0.0,\n",
        "        d_out=1,\n",
        "        d_ffn_factor=4/3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_num_features = n_num_features\n",
        "        self.cat_cardinalities = cat_cardinalities or []\n",
        "        self.d_token = d_token\n",
        "        self.n_blocks = n_blocks\n",
        "        self.d_out = d_out\n",
        "        \n",
        "        # Feature tokenization: each numerical feature gets its own token\n",
        "        # Linear layer to map each feature value to d_token dimensions\n",
        "        self.num_embedding = nn.Linear(1, d_token)\n",
        "        \n",
        "        # CLS token (learnable token for aggregation)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
        "        \n",
        "        # Positional embeddings\n",
        "        max_num_tokens = n_num_features + len(self.cat_cardinalities) + 1  # +1 for CLS\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_num_tokens, d_token))\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_token=d_token,\n",
        "                attention_dropout=attention_dropout,\n",
        "                ffn_dropout=ffn_dropout,\n",
        "                residual_dropout=residual_dropout,\n",
        "                d_ffn_factor=d_ffn_factor,\n",
        "            )\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "        \n",
        "        # Classification head\n",
        "        self.head = nn.Linear(d_token, d_out)\n",
        "        \n",
        "    def forward(self, x_num, x_cat=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_num: numerical features [batch_size, n_num_features]\n",
        "            x_cat: categorical features [batch_size, n_cat_features] (optional)\n",
        "        \"\"\"\n",
        "        batch_size = x_num.shape[0]\n",
        "        \n",
        "        # Tokenize numerical features: each feature becomes a token\n",
        "        # Reshape to [batch_size * n_num_features, 1] then embed\n",
        "        num_features_reshaped = x_num.unsqueeze(-1)  # [batch_size, n_num_features, 1]\n",
        "        num_tokens = self.num_embedding(num_features_reshaped)  # [batch_size, n_num_features, d_token]\n",
        "        \n",
        "        # Combine tokens\n",
        "        tokens = [num_tokens]\n",
        "        \n",
        "        # Add CLS token\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [batch_size, 1, d_token]\n",
        "        tokens = [cls_tokens] + tokens\n",
        "        \n",
        "        # Concatenate all tokens\n",
        "        x = torch.cat(tokens, dim=1)  # [batch_size, n_tokens, d_token]\n",
        "        \n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding[:, :x.shape[1], :]\n",
        "        \n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        # Use CLS token for prediction\n",
        "        cls_output = x[:, 0, :]  # [batch_size, d_token]\n",
        "        \n",
        "        # Classification head\n",
        "        output = self.head(cls_output)  # [batch_size, d_out]\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with multi-head attention and FFN\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_token,\n",
        "        attention_dropout=0.2,\n",
        "        ffn_dropout=0.1,\n",
        "        residual_dropout=0.0,\n",
        "        d_ffn_factor=4/3,\n",
        "        n_heads=8,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=d_token,\n",
        "            num_heads=n_heads,\n",
        "            dropout=attention_dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.ffn = FeedForward(d_token, int(d_token * d_ffn_factor), ffn_dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_token)\n",
        "        self.ln2 = nn.LayerNorm(d_token)\n",
        "        self.residual_dropout = residual_dropout\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        attn_out, _ = self.attention(x, x, x)\n",
        "        if self.residual_dropout > 0:\n",
        "            attn_out = F.dropout(attn_out, p=self.residual_dropout, training=self.training)\n",
        "        x = self.ln1(x + attn_out)\n",
        "        \n",
        "        # FFN with residual connection\n",
        "        ffn_out = self.ffn(x)\n",
        "        if self.residual_dropout > 0:\n",
        "            ffn_out = F.dropout(ffn_out, p=self.residual_dropout, training=self.training)\n",
        "        x = self.ln2(x + ffn_out)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Feed-forward network\"\"\"\n",
        "    def __init__(self, d_in, d_hidden, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_in, d_hidden)\n",
        "        self.linear2 = nn.Linear(d_hidden, d_in)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "print(\"✓ Custom FT-Transformer implementation loaded (PyTorch 2.x compatible)\")\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    f1_score, precision_score, recall_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"✓ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Datasets\n",
        "\n",
        "Load the pre-filtered datasets that already have evidence features encoded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Datasets loaded successfully!\n",
            "  Training: (936888, 592)\n",
            "  Validation: (129258, 592)\n",
            "  Test: (142184, 592)\n",
            "  Total pathologies: 49\n"
          ]
        }
      ],
      "source": [
        "# Load filtered datasets\n",
        "base_path = \"/Users/zeynep_yilmaz/Desktop/sdp_gregDDx/DDxPlus Dataset/preprocessed_filtered/\"\n",
        "train_df = pd.read_csv(f\"{base_path}train_filtered.csv\")\n",
        "val_df = pd.read_csv(f\"{base_path}validation_filtered.csv\")\n",
        "test_df = pd.read_csv(f\"{base_path}test_filtered.csv\")\n",
        "\n",
        "# Drop raw 'SEX' and 'DIFFERENTIAL_DIAGNOSIS' columns to prevent leakage\n",
        "train_df = train_df.drop(columns=['SEX', 'DIFFERENTIAL_DIAGNOSIS'], errors='ignore')\n",
        "val_df = val_df.drop(columns=['SEX', 'DIFFERENTIAL_DIAGNOSIS'], errors='ignore')\n",
        "test_df = test_df.drop(columns=['SEX', 'DIFFERENTIAL_DIAGNOSIS'], errors='ignore')\n",
        "\n",
        "# Load label encoder\n",
        "with open(\"/Users/zeynep_yilmaz/Desktop/sdp_gregDDx/DDxPlus Dataset/pkl files/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "print(\"✓ Datasets loaded successfully!\")\n",
        "print(f\"  Training: {train_df.shape}\")\n",
        "print(f\"  Validation: {val_df.shape}\")\n",
        "print(f\"  Test: {test_df.shape}\")\n",
        "print(f\"  Total pathologies: {len(label_encoder.classes_)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection\n",
        "\n",
        "Select features for training. Exclude non-feature columns and prepare X and y.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total columns: 592\n",
            "Selected features: 590\n",
            "Feature breakdown:\n",
            "  - Demographics: 2 (['AGE', 'SEX_ENCODED'])\n",
            "  - Evidence features: 492\n",
            "  - Initial evidence features: 96\n",
            "Total features: 590\n",
            "✓ Data prepared:\n",
            "  X_train: (936888, 590) | X_val: (129258, 590) | X_test: (142184, 590)\n",
            "✓ Features standardized for deep learning models\n"
          ]
        }
      ],
      "source": [
        "# Feature selection + breakdown + data prep (uses filtered datasets)\n",
        "non_feature_cols = ['PATHOLOGY', 'PATHOLOGY_ENCODED'] # SEX and DIFFERENTIAL_DIAGNOSIS already dropped\n",
        "feature_cols = [col for col in train_df.columns if col not in non_feature_cols]\n",
        "\n",
        "print(f'Total columns: {train_df.shape[1]}')\n",
        "print(f'Selected features: {len(feature_cols)}')\n",
        "\n",
        "demo_features = [c for c in feature_cols if c in ['AGE', 'SEX_ENCODED']]\n",
        "evidence_features = [c for c in feature_cols if c.startswith('evidence_')]\n",
        "initial_features = [c for c in feature_cols if c.startswith('initial_')]\n",
        "\n",
        "print('Feature breakdown:')\n",
        "print(f'  - Demographics: {len(demo_features)} ({demo_features})')\n",
        "print(f'  - Evidence features: {len(evidence_features)}')\n",
        "print(f'  - Initial evidence features: {len(initial_features)}')\n",
        "print(f'Total features: {len(feature_cols)}')\n",
        "\n",
        "# Prepare X and y\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df['PATHOLOGY_ENCODED'].values\n",
        "X_val = val_df[feature_cols].values\n",
        "y_val = val_df['PATHOLOGY_ENCODED'].values\n",
        "X_test = test_df[feature_cols].values\n",
        "y_test = test_df['PATHOLOGY_ENCODED'].values\n",
        "\n",
        "print('✓ Data prepared:')\n",
        "print(f'  X_train: {X_train.shape} | X_val: {X_val.shape} | X_test: {X_test.shape}')\n",
        "\n",
        "# Standardize features for deep learning models\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print('✓ Features standardized for deep learning models')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1: TabNet\n",
        "\n",
        "TabNet uses sequential attention to learn interpretable representations and can achieve state-of-the-art performance on tabular data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training TabNet...\n",
            "epoch 0  | loss: 0.20061 | train_accuracy: 0.9948  | val_accuracy: 0.99524 |  0:12:21s\n",
            "epoch 1  | loss: 0.01551 | train_accuracy: 0.99615 | val_accuracy: 0.99627 |  0:25:09s\n",
            "epoch 2  | loss: 0.01788 | train_accuracy: 0.99672 | val_accuracy: 0.99685 |  0:37:14s\n",
            "epoch 3  | loss: 0.01173 | train_accuracy: 0.99671 | val_accuracy: 0.99677 |  0:50:05s\n",
            "epoch 4  | loss: 0.01551 | train_accuracy: 0.99672 | val_accuracy: 0.99688 |  1:02:57s\n",
            "epoch 5  | loss: 0.01312 | train_accuracy: 0.99669 | val_accuracy: 0.99672 |  1:11:44s\n",
            "epoch 6  | loss: 0.0106  | train_accuracy: 0.99695 | val_accuracy: 0.99695 |  1:15:52s\n",
            "epoch 7  | loss: 0.01023 | train_accuracy: 0.997   | val_accuracy: 0.99703 |  1:20:02s\n",
            "epoch 8  | loss: 0.01053 | train_accuracy: 0.99697 | val_accuracy: 0.99704 |  1:24:25s\n",
            "epoch 9  | loss: 0.01099 | train_accuracy: 0.9971  | val_accuracy: 0.99712 |  1:28:25s\n",
            "epoch 10 | loss: 0.00983 | train_accuracy: 0.9969  | val_accuracy: 0.99698 |  1:32:31s\n",
            "epoch 11 | loss: 0.00978 | train_accuracy: 0.99701 | val_accuracy: 0.99711 |  1:36:34s\n",
            "epoch 12 | loss: 0.01001 | train_accuracy: 0.99654 | val_accuracy: 0.99684 |  1:40:42s\n",
            "epoch 13 | loss: 0.00982 | train_accuracy: 0.99702 | val_accuracy: 0.99699 |  1:44:44s\n",
            "epoch 14 | loss: 0.0101  | train_accuracy: 0.99707 | val_accuracy: 0.99698 |  1:49:18s\n",
            "epoch 15 | loss: 0.00946 | train_accuracy: 0.99697 | val_accuracy: 0.99702 |  1:53:36s\n",
            "epoch 16 | loss: 0.00964 | train_accuracy: 0.99711 | val_accuracy: 0.99707 |  1:57:59s\n",
            "epoch 17 | loss: 0.00964 | train_accuracy: 0.99705 | val_accuracy: 0.99709 |  2:03:27s\n",
            "epoch 18 | loss: 0.00935 | train_accuracy: 0.99708 | val_accuracy: 0.99707 |  2:15:50s\n",
            "epoch 19 | loss: 0.00904 | train_accuracy: 0.99719 | val_accuracy: 0.99706 |  2:28:52s\n",
            "epoch 20 | loss: 0.00936 | train_accuracy: 0.99704 | val_accuracy: 0.99688 |  2:40:55s\n",
            "epoch 21 | loss: 0.00918 | train_accuracy: 0.99707 | val_accuracy: 0.99705 |  2:53:13s\n"
          ]
        }
      ],
      "source": [
        "if TabNetClassifier is None:\n",
        "    print(\"❌ TabNet not available. Please install pytorch-tabnet: pip install pytorch-tabnet\")\n",
        "else:\n",
        "    # TabNet parameters\n",
        "    tabnet_params = {\n",
        "        'n_d': 64,              # Dimension of the decision layer\n",
        "        'n_a': 64,              # Dimension of the attention layer\n",
        "        'n_steps': 5,            # Number of steps in the encoder\n",
        "        'gamma': 1.5,            # Coefficient for feature reusage\n",
        "        'lambda_sparse': 1e-3,   # Sparsity regularization\n",
        "        'optimizer_fn': torch.optim.Adam,\n",
        "        'optimizer_params': dict(lr=2e-2),\n",
        "        'mask_type': 'entmax',   # How to use attention\n",
        "        'n_shared': 2,           # Number of shared GLU layers\n",
        "        'n_independent': 2,      # Number of independent GLU layers per step\n",
        "        'clip_value': 1.0,       # Gradient clipping\n",
        "        'verbose': 1,\n",
        "        'seed': 42,\n",
        "        'device_name': 'auto'    # 'cpu' or 'cuda' or 'auto'\n",
        "    }\n",
        "    \n",
        "    print(\"Training TabNet...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    tabnet_model = TabNetClassifier(**tabnet_params)\n",
        "    \n",
        "    tabnet_model.fit(\n",
        "        X_train=X_train_scaled,\n",
        "        y_train=y_train,\n",
        "        eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
        "        eval_name=['train', 'val'],\n",
        "        eval_metric=['accuracy'],\n",
        "        max_epochs=100,\n",
        "        patience=15,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False\n",
        "    )\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"✓ TabNet trained in {training_time:.2f} seconds\")\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_proba = tabnet_model.predict_proba(X_val_scaled)\n",
        "    y_val_pred = tabnet_model.predict(X_val_scaled)\n",
        "    \n",
        "    # Evaluate\n",
        "    tabnet_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    tabnet_f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
        "    tabnet_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
        "    top3_pred = np.argsort(y_val_pred_proba, axis=1)[:, -3:]\n",
        "    top3_accuracy = np.mean([y_val[i] in top3_pred[i] for i in range(len(y_val))])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TABNET VALIDATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Accuracy: {tabnet_accuracy:.4f}\")\n",
        "    print(f\"Macro F1-score: {tabnet_f1_macro:.4f}\")\n",
        "    print(f\"Weighted F1-score: {tabnet_f1_weighted:.4f}\")\n",
        "    print(f\"Top-3 Accuracy: {top3_accuracy:.4f}\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    \n",
        "    tabnet_results = {\n",
        "        'accuracy': tabnet_accuracy,\n",
        "        'f1_macro': tabnet_f1_macro,\n",
        "        'f1_weighted': tabnet_f1_weighted,\n",
        "        'top3_accuracy': top3_accuracy,\n",
        "        'training_time': training_time\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2: FT-Transformer\n",
        "\n",
        "FT-Transformer applies the Transformer architecture to tabular data by tokenizing features and using self-attention mechanisms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (496945516.py, line 6)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mn_features = X_train_scaled.shape[1]\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# Convert to PyTorch tensors\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "    \n",
        "    # FT-Transformer configuration\n",
        "    n_features = X_train_scaled.shape[1]\n",
        "    n_classes = len(label_encoder.classes_)\n",
        "    \n",
        "    model_config = {\n",
        "        'n_num_features': n_features,  # All features are numerical (after encoding)\n",
        "        'cat_cardinalities': [],  # No categorical features (already encoded)\n",
        "        'd_token': 192,           # Dimension of feature tokens\n",
        "        'n_blocks': 3,            # Number of transformer blocks\n",
        "        'attention_dropout': 0.2,\n",
        "        'ffn_dropout': 0.1,\n",
        "        'residual_dropout': 0.0,\n",
        "        'd_out': n_classes        # Output dimension (number of classes)\n",
        "    }\n",
        "    \n",
        "    print(\"Training FT-Transformer...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create model\n",
        "    ft_model = FTTransformer(**model_config).to(device)\n",
        "    \n",
        "    # Training parameters\n",
        "    learning_rate = 1e-4\n",
        "    batch_size = 512\n",
        "    n_epochs = 50\n",
        "    patience = 10\n",
        "    \n",
        "    # Optimizer and loss\n",
        "    optimizer = torch.optim.AdamW(ft_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
        "    y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "    \n",
        "    # Create DataLoader for batching\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        # Training\n",
        "        ft_model.train()\n",
        "        epoch_train_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - FT-Transformer expects X_num and optionally X_cat\n",
        "            # Since all features are numerical, pass X_num and None for X_cat\n",
        "            output = ft_model(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_train_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_train_loss = epoch_train_loss / num_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        ft_model.eval()\n",
        "        with torch.no_grad():\n",
        "            # For validation, we process in batches to avoid memory issues\n",
        "            val_outputs = []\n",
        "            val_batch_size = 1024\n",
        "            for i in range(0, len(X_val_tensor), val_batch_size):\n",
        "                batch_X_val = X_val_tensor[i:i+val_batch_size]\n",
        "                batch_output = ft_model(batch_X_val)\n",
        "                val_outputs.append(batch_output)\n",
        "            \n",
        "            val_output = torch.cat(val_outputs, dim=0)\n",
        "            val_loss = criterion(val_output, y_val_tensor)\n",
        "            val_losses.append(val_loss.item())\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            val_pred = torch.argmax(val_output, dim=1)\n",
        "            val_acc = (val_pred == y_val_tensor).float().mean().item()\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            best_model_state = ft_model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
        "                  f\"Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    # Load best model\n",
        "    ft_model.load_state_dict(best_model_state)\n",
        "    ft_model.eval()\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"✓ FT-Transformer trained in {training_time:.2f} seconds\")\n",
        "    \n",
        "    # Predictions on validation set\n",
        "    with torch.no_grad():\n",
        "        val_outputs = []\n",
        "        val_batch_size = 1024\n",
        "        for i in range(0, len(X_val_tensor), val_batch_size):\n",
        "            batch_X_val = X_val_tensor[i:i+val_batch_size]\n",
        "            batch_output = ft_model(batch_X_val)\n",
        "            val_outputs.append(batch_output)\n",
        "        \n",
        "        val_output = torch.cat(val_outputs, dim=0)\n",
        "        y_val_pred_proba = torch.softmax(val_output, dim=1).cpu().numpy()\n",
        "        y_val_pred = torch.argmax(val_output, dim=1).cpu().numpy()\n",
        "    \n",
        "    # Evaluate\n",
        "    ft_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    ft_f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
        "    ft_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
        "    top3_pred = np.argsort(y_val_pred_proba, axis=1)[:, -3:]\n",
        "    top3_accuracy = np.mean([y_val[i] in top3_pred[i] for i in range(len(y_val))])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FT-TRANSFORMER VALIDATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Accuracy: {ft_accuracy:.4f}\")\n",
        "    print(f\"Macro F1-score: {ft_f1_macro:.4f}\")\n",
        "    print(f\"Weighted F1-score: {ft_f1_weighted:.4f}\")\n",
        "    print(f\"Top-3 Accuracy: {top3_accuracy:.4f}\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    \n",
        "    ft_results = {\n",
        "        'accuracy': ft_accuracy,\n",
        "        'f1_macro': ft_f1_macro,\n",
        "        'f1_weighted': ft_f1_weighted,\n",
        "        'top3_accuracy': top3_accuracy,\n",
        "        'training_time': training_time\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Evaluation\n",
        "\n",
        "Now let's evaluate all models on the true test set to get final results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEST SET EVALUATION\n",
            "================================================================================\n",
            "\n",
            "⚠️  No models available for test evaluation\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "print(\"=\"*80)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_results = {}\n",
        "\n",
        "# TabNet test predictions\n",
        "if TabNetClassifier is not None and 'tabnet_model' in locals():\n",
        "    print(\"\\nEvaluating TabNet on test set...\")\n",
        "    y_test_pred_proba_tabnet = tabnet_model.predict_proba(X_test_scaled)\n",
        "    y_test_pred_tabnet = tabnet_model.predict(X_test_scaled)\n",
        "    \n",
        "    test_acc_tabnet = accuracy_score(y_test, y_test_pred_tabnet)\n",
        "    test_f1_macro_tabnet = f1_score(y_test, y_test_pred_tabnet, average='macro')\n",
        "    test_f1_weighted_tabnet = f1_score(y_test, y_test_pred_tabnet, average='weighted')\n",
        "    top3_pred_tabnet = np.argsort(y_test_pred_proba_tabnet, axis=1)[:, -3:]\n",
        "    top3_acc_tabnet = np.mean([y_test[i] in top3_pred_tabnet[i] for i in range(len(y_test))])\n",
        "    \n",
        "    test_results['TabNet'] = {\n",
        "        'accuracy': test_acc_tabnet,\n",
        "        'f1_macro': test_f1_macro_tabnet,\n",
        "        'f1_weighted': test_f1_weighted_tabnet,\n",
        "        'top3_accuracy': top3_acc_tabnet\n",
        "    }\n",
        "    \n",
        "    print(f\"TabNet Test Results:\")\n",
        "    print(f\"  Accuracy: {test_acc_tabnet:.4f}\")\n",
        "    print(f\"  Macro F1: {test_f1_macro_tabnet:.4f}\")\n",
        "    print(f\"  Weighted F1: {test_f1_weighted_tabnet:.4f}\")\n",
        "    print(f\"  Top-3 Accuracy: {top3_acc_tabnet:.4f}\")\n",
        "\n",
        "# FT-Transformer test predictions\n",
        "if 'ft_model' in locals():\n",
        "    print(\"\\nEvaluating FT-Transformer on test set...\")\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "    \n",
        "    ft_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Process test set in batches\n",
        "        test_outputs = []\n",
        "        test_batch_size = 1024\n",
        "        for i in range(0, len(X_test_tensor), test_batch_size):\n",
        "            batch_X_test = X_test_tensor[i:i+test_batch_size]\n",
        "            batch_output = ft_model(batch_X_test)\n",
        "            test_outputs.append(batch_output)\n",
        "        \n",
        "        test_output = torch.cat(test_outputs, dim=0)\n",
        "        y_test_pred_proba_ft = torch.softmax(test_output, dim=1).cpu().numpy()\n",
        "        y_test_pred_ft = torch.argmax(test_output, dim=1).cpu().numpy()\n",
        "    \n",
        "    test_acc_ft = accuracy_score(y_test, y_test_pred_ft)\n",
        "    test_f1_macro_ft = f1_score(y_test, y_test_pred_ft, average='macro')\n",
        "    test_f1_weighted_ft = f1_score(y_test, y_test_pred_ft, average='weighted')\n",
        "    top3_pred_ft = np.argsort(y_test_pred_proba_ft, axis=1)[:, -3:]\n",
        "    top3_acc_ft = np.mean([y_test[i] in top3_pred_ft[i] for i in range(len(y_test))])\n",
        "    \n",
        "    test_results['FT-Transformer'] = {\n",
        "        'accuracy': test_acc_ft,\n",
        "        'f1_macro': test_f1_macro_ft,\n",
        "        'f1_weighted': test_f1_weighted_ft,\n",
        "        'top3_accuracy': top3_acc_ft\n",
        "    }\n",
        "    \n",
        "    print(f\"FT-Transformer Test Results:\")\n",
        "    print(f\"  Accuracy: {test_acc_ft:.4f}\")\n",
        "    print(f\"  Macro F1: {test_f1_macro_ft:.4f}\")\n",
        "    print(f\"  Weighted F1: {test_f1_weighted_ft:.4f}\")\n",
        "    print(f\"  Top-3 Accuracy: {top3_acc_ft:.4f}\")\n",
        "\n",
        "# Create test comparison\n",
        "if test_results:\n",
        "    test_comparison_data = {\n",
        "        'Model': list(test_results.keys()),\n",
        "        'Accuracy': [test_results[m]['accuracy'] for m in test_results.keys()],\n",
        "        'Macro F1': [test_results[m]['f1_macro'] for m in test_results.keys()],\n",
        "        'Weighted F1': [test_results[m]['f1_weighted'] for m in test_results.keys()],\n",
        "        'Top-3 Accuracy': [test_results[m]['top3_accuracy'] for m in test_results.keys()]\n",
        "    }\n",
        "    \n",
        "    test_comparison = pd.DataFrame(test_comparison_data)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TEST SET COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    print(test_comparison.to_string(index=False))\n",
        "else:\n",
        "    print(\"\\n⚠️  No models available for test evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison\n",
        "\n",
        "Compare validation set performance across models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  No models available for comparison\n"
          ]
        }
      ],
      "source": [
        "# Create comparison\n",
        "comparison_data = []\n",
        "if TabNetClassifier is not None and 'tabnet_results' in locals():\n",
        "    comparison_data.append({\n",
        "        'Model': 'TabNet',\n",
        "        'Accuracy': tabnet_results['accuracy'],\n",
        "        'Macro F1': tabnet_results['f1_macro'],\n",
        "        'Weighted F1': tabnet_results['f1_weighted'],\n",
        "        'Top-3 Accuracy': tabnet_results['top3_accuracy'],\n",
        "        'Training Time (s)': tabnet_results['training_time']\n",
        "    })\n",
        "\n",
        "if 'ft_results' in locals():\n",
        "    comparison_data.append({\n",
        "        'Model': 'FT-Transformer',\n",
        "        'Accuracy': ft_results['accuracy'],\n",
        "        'Macro F1': ft_results['f1_macro'],\n",
        "        'Weighted F1': ft_results['f1_weighted'],\n",
        "        'Top-3 Accuracy': ft_results['top3_accuracy'],\n",
        "        'Training Time (s)': ft_results['training_time']\n",
        "    })\n",
        "\n",
        "if comparison_data:\n",
        "    comparison = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"MODEL COMPARISON (VALIDATION SET)\")\n",
        "    print(\"=\"*80)\n",
        "    print(comparison.to_string(index=False))\n",
        "    \n",
        "    # Visualize\n",
        "    if len(comparison) > 0:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Tabular Deep Learning Models Comparison', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        metrics = ['Accuracy', 'Macro F1', 'Weighted F1', 'Top-3 Accuracy']\n",
        "        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "        \n",
        "        for i, metric in enumerate(metrics):\n",
        "            row = i // 2\n",
        "            col = i % 2\n",
        "            ax = axes[row, col]\n",
        "            \n",
        "            bars = ax.bar(comparison['Model'], comparison[metric], \n",
        "                         color=colors[:len(comparison)])\n",
        "            ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel(metric)\n",
        "            ax.set_ylim([0, 1])\n",
        "            ax.grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            for j, v in enumerate(comparison[metric]):\n",
        "                ax.text(j, v, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save plot\n",
        "        results_dir = Path('../results')\n",
        "        results_dir.mkdir(exist_ok=True)\n",
        "        plt.savefig(results_dir / 'tabular_dl_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"\\n✓ Comparison plot saved to {results_dir / 'tabular_dl_comparison.png'}\")\n",
        "else:\n",
        "    print(\"⚠️  No models available for comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Saving\n",
        "\n",
        "Save trained models for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Results and preprocessing objects saved to ../saved_models/tabular_dl_results.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save models\n",
        "saved_models_dir = Path('../saved_models')\n",
        "saved_models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save TabNet\n",
        "if TabNetClassifier is not None and 'tabnet_model' in locals():\n",
        "    tabnet_save_path = saved_models_dir / 'tabnet_model'\n",
        "    tabnet_model.save_model(str(tabnet_save_path))\n",
        "    print(f\"✓ TabNet model saved to {tabnet_save_path}\")\n",
        "\n",
        "# Save FT-Transformer\n",
        "if 'ft_model' in locals():\n",
        "    ft_save_path = saved_models_dir / 'ft_transformer_model.pth'\n",
        "    torch.save({\n",
        "        'model_state_dict': ft_model.state_dict(),\n",
        "        'model_config': model_config,\n",
        "        'scaler': scaler,\n",
        "        'label_encoder': label_encoder\n",
        "    }, ft_save_path)\n",
        "    print(f\"✓ FT-Transformer model saved to {ft_save_path}\")\n",
        "\n",
        "# Save scaler and results\n",
        "results_dict = {\n",
        "    'scaler': scaler,\n",
        "    'label_encoder': label_encoder,\n",
        "    'feature_cols': feature_cols,\n",
        "    'validation_results': {}\n",
        "}\n",
        "\n",
        "if 'tabnet_results' in locals():\n",
        "    results_dict['validation_results']['TabNet'] = tabnet_results\n",
        "if 'ft_results' in locals():\n",
        "    results_dict['validation_results']['FT-Transformer'] = ft_results\n",
        "if test_results:\n",
        "    results_dict['test_results'] = test_results\n",
        "\n",
        "results_path = saved_models_dir / 'tabular_dl_results.pkl'\n",
        "with open(results_path, 'wb') as f:\n",
        "    pickle.dump(results_dict, f)\n",
        "print(f\"✓ Results and preprocessing objects saved to {results_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
