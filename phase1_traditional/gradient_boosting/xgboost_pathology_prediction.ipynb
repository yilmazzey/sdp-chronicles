{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for Pathology Prediction\n",
    "\n",
    "## Overview\n",
    "This notebook implements XGBoost (Extreme Gradient Boosting) for multi-class pathology prediction using the DDxPlus dataset.\n",
    "\n",
    "## Objectives\n",
    "- Load preprocessed training, validation, and test data\n",
    "- Train XGBoost model for pathology classification\n",
    "- Evaluate model performance with comprehensive metrics\n",
    "- Visualize feature importance and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-3.1.1-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting numpy (from xgboost)\n",
      "  Using cached numpy-2.3.4-cp314-cp314-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scipy (from xgboost)\n",
      "  Using cached scipy-1.16.3-cp314-cp314-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Using cached xgboost-3.1.1-py3-none-macosx_12_0_arm64.whl (2.2 MB)\n",
      "Using cached numpy-2.3.4-cp314-cp314-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached scipy-1.16.3-cp314-cp314-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Installing collected packages: numpy, scipy, xgboost\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [xgboost]m2/3\u001b[0m [xgboost]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.4 scipy-1.16.3 xgboost-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /Users/denizcii/Desktop/sdp-chronicles/.venv/lib/python3.14/site-packages (2.3.4)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp314-cp314-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp314-cp314-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/denizcii/Desktop/sdp-chronicles/.venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.1-cp314-cp314-macosx_10_13_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp314-cp314-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/denizcii/Desktop/sdp-chronicles/.venv/lib/python3.14/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-12.0.0-cp314-cp314-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/denizcii/Desktop/sdp-chronicles/.venv/lib/python3.14/site-packages (from scikit-learn) (1.16.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/denizcii/Desktop/sdp-chronicles/.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached matplotlib-3.10.7-cp314-cp314-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached scikit_learn-1.7.2-cp314-cp314-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached contourpy-1.3.3-cp314-cp314-macosx_11_0_arm64.whl (273 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp314-cp314-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached kiwisolver-1.4.9-cp314-cp314-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached pillow-12.0.0-cp314-cp314-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, pandas, matplotlib, seaborn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [seaborn]3/14\u001b[0m [seaborn]ib]n]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.7 pandas-2.3.3 pillow-12.0.0 pyparsing-3.2.5 pytz-2025.2 scikit-learn-1.7.2 seaborn-0.13.2 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "XGBoost version: 3.1.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load label encoder\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m../../DDxPlus Dataset/pkl files/label_encoder.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     label_encoder = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Data loaded successfully from preprocessed_stratified directory\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDataset shapes:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnpicklingError\u001b[39m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "# Load preprocessed datasets from stratified directory (features already prepared)\n",
    "train_df = pd.read_csv('../../DDxPlus Dataset/preprocessed_stratified/train_preprocessed.csv')\n",
    "val_df = pd.read_csv('../../DDxPlus Dataset/preprocessed_stratified/validation_preprocessed.csv')\n",
    "test_df = pd.read_csv('../../DDxPlus Dataset/preprocessed_stratified/test_preprocessed.csv')\n",
    "\n",
    "# Load label encoder\n",
    "with open('../../DDxPlus Dataset/pkl files/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "print(\"✓ Data loaded successfully from preprocessed_stratified directory\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  Training:   {train_df.shape}\")\n",
    "print(f\"  Validation: {val_df.shape}\")\n",
    "print(f\"  Test:       {test_df.shape}\")\n",
    "print(f\"\\nNumber of pathologies: {len(label_encoder.classes_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (exclude PATHOLOGY and other non-feature columns)\n",
    "exclude_cols = ['PATHOLOGY', 'EVIDENCES', 'DIFFERENTIAL_DIAGNOSIS']\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  - Demographics: {[col for col in feature_cols if col in ['AGE', 'SEX_ENCODED']]}\")\n",
    "print(f\"  - Evidence features: {len([col for col in feature_cols if col.startswith('evidence_')])}\")\n",
    "print(f\"  - Other features: {len([col for col in feature_cols if col not in ['AGE', 'SEX_ENCODED'] and not col.startswith('evidence_')])}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['PATHOLOGY_ENCODED']\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['PATHOLOGY_ENCODED']\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['PATHOLOGY_ENCODED']\n",
    "\n",
    "print(f\"\\n✓ Data prepared:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  X_val shape: {X_val.shape}\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',  # Multi-class classification with probability\n",
    "    'num_class': num_classes,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist'  # Faster training\n",
    "}\n",
    "\n",
    "print(\"XGBoost Configuration:\")\n",
    "for key, value in xgb_params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train XGBoost Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "print(\"Training XGBoost model...\")\n",
    "print(\"This may take several minutes depending on your system...\")\n",
    "\n",
    "# Train the model with early stopping\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation and test sets\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Validation Set:\")\n",
    "print(f\"  Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(f\"  F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 Test Set:\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for test set\n",
    "print(\"\\nDetailed Classification Report (Test Set):\")\n",
    "print(\"=\" * 80)\n",
    "report = classification_report(y_test, y_test_pred, \n",
    "                              target_names=label_encoder.classes_,\n",
    "                              output_dict=True)\n",
    "\n",
    "# Print report for a few pathologies\n",
    "print(\"\\nTop 10 Most Common Pathologies:\")\n",
    "pathology_counts = test_df['PATHOLOGY'].value_counts()\n",
    "for patho in pathology_counts.head(10).index:\n",
    "    patho_encoded = label_encoder.transform([patho])[0]\n",
    "    if patho_encoded in report:\n",
    "        print(f\"\\n{patho}:\")\n",
    "        print(f\"  Precision: {report[str(patho_encoded)]['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {report[str(patho_encoded)]['recall']:.4f}\")\n",
    "        print(f\"  F1-Score:  {report[str(patho_encoded)]['f1-score']:.4f}\")\n",
    "        print(f\"  Support:   {report[str(patho_encoded)]['support']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for test set\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=False, yticklabels=False)\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Pathology', fontsize=12)\n",
    "plt.xlabel('Predicted Pathology', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "print(f\"\\nBest performing pathology: {class_accuracies.max():.2%}\")\n",
    "print(f\"Worst performing pathology: {class_accuracies.min():.2%}\")\n",
    "print(f\"Mean per-class accuracy: {class_accuracies.mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(xgb_model, 'xgboost_pathology_model.pkl')\n",
    "print(\"✓ Model saved as 'xgboost_pathology_model.pkl'\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('xgboost_feature_importance.csv', index=False)\n",
    "print(\"✓ Feature importance saved as 'xgboost_feature_importance.csv'\")\n",
    "\n",
    "# Save evaluation metrics\n",
    "metrics = {\n",
    "    'val_accuracy': val_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'val_f1': val_f1,\n",
    "    'test_f1': test_f1\n",
    "}\n",
    "\n",
    "with open('xgboost_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "print(\"✓ Metrics saved as 'xgboost_metrics.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the use of XGBoost for pathology prediction on the DDxPlus dataset. The model achieves competitive performance through:\n",
    "\n",
    "- **Feature Engineering**: Using demographics, evidence counts, and binary evidence features\n",
    "- **XGBoost Optimization**: Tuned hyperparameters for multi-class classification\n",
    "- **Evaluation**: Comprehensive metrics including accuracy, F1-score, and per-class performance\n",
    "- **Analysis**: Feature importance insights and confusion matrix visualization\n",
    "\n",
    "### Key Results\n",
    "- Model saved as `xgboost_pathology_model.pkl`\n",
    "- Feature importance saved as `xgboost_feature_importance.csv`\n",
    "- Evaluation metrics saved as `xgboost_metrics.pkl`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
